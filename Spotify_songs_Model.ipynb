{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlZKOfb3g8op"
      },
      "source": [
        "##Spotify Most Streamed Songs EDA & Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9LZjqAKhnFg"
      },
      "source": [
        "**Main goal of a project** is to make an EDA of dataset and to find which regression model will represent better on this data.\n",
        "\n",
        "**Technologies & library used:** pandas, numpy, seaborn, plotly, matplotlib for analysis and plotting; Random Forest, Gradient Boosting Machines (GBM), Support Vector Machines (SVM), Logistic Regression and TensorFlow Keras Neural Networks for models.\n",
        "\n",
        "**Main info about dataset:**\n",
        "\n",
        "Dataset is from [Kaggle](https://www.kaggle.com/datasets/amaanansari09/most-streamed-songs-all-time) and it shows 100 most streamed songs on Spotify with their Features Extracted using the Spotify API. \n",
        "\n",
        "Firstly it consists of 2 datasets 'features' and 'streams'. \n",
        "\n",
        "Dataset 'Features' has columns such as:\n",
        "* *id* - Unique ID given to each song on spotify;\n",
        "* *name* - name of the song;\n",
        "* *duration* - duration of the song in minutes\n",
        "* *energy* - measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n",
        "* *key* - the key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = Câ™¯/Dâ™­, 2 = D, and so on. If no key was detected, the value is -1.\n",
        "* *loudness* - the overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\n",
        "* *mode* - indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n",
        "* *speechiness* - detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n",
        "* *acousticness* - a confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n",
        "* *instrumentalness* - predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n",
        "* *liveness* - represents the probability that a song was performed live. A value close to 1 indicates a high likelihood of the song being performed live, while a value close to 0 suggests a studio recording.\n",
        "* *valence*\t- represents the musical positiveness conveyed by a song. A higher valence value indicates a more positive and cheerful tone, while a lower value represents a more negative or sad tone.\n",
        "* *tempo*\t- represents the tempo or speed of a song measured in beats per minute (BPM). It indicates the overall pace and rhythm of the music.\n",
        "* *danceability* - measures the suitability of a song for dancing based on factors like rhythm, beat strength, and tempo. A higher danceability value suggests a song that is more suitable for dancing, while a lower value indicates a song that may be less danceable.\n",
        "\n",
        "Dataset 'Streams' has such columns:\n",
        "* *song* - name of the song\n",
        "* *artist* - name of the artist\n",
        "* *streams(billions)* - streams of the song in billions \n",
        "* release date - date of the song release\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Importing libraries"
      ],
      "metadata": {
        "id": "gJpXWeDAFMJp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IngNKv9qgk4D"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Importing dataset and merging it"
      ],
      "metadata": {
        "id": "WFK8-SCMFP-O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z1f2IDRTjA5x"
      },
      "outputs": [],
      "source": [
        "features = pd.read_csv('/kaggle/input/most-streamed-songs-all-time/Features.csv')\n",
        "streams = pd.read_csv('/kaggle/input/most-streamed-songs-all-time/Streams.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JklVC2oyjLCF"
      },
      "outputs": [],
      "source": [
        "features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDIhfKtojOm1"
      },
      "outputs": [],
      "source": [
        "streams.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcnjhUMxz2dn"
      },
      "outputs": [],
      "source": [
        "df_main = pd.merge(left=streams, left_on = 'Song',\n",
        "                   right = features, right_on = 'name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrycTvDC1Cm1"
      },
      "outputs": [],
      "source": [
        "df_main.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Basic data analysis"
      ],
      "metadata": {
        "id": "asrQTV2FFZg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping column 'name' which is duplicate of the column 'song'"
      ],
      "metadata": {
        "id": "4nvWTfiyFedg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7BYmPC51cVw"
      },
      "outputs": [],
      "source": [
        "df_main = df_main.drop(columns = ['name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking missing values"
      ],
      "metadata": {
        "id": "d4Z-uarOFpGn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeBntzLICpFJ"
      },
      "outputs": [],
      "source": [
        "print(f'Amount of missing values:\\n{df_main.isna().sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking duplicates"
      ],
      "metadata": {
        "id": "N0qXAc05FrCB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZfS-OoTkZfa"
      },
      "outputs": [],
      "source": [
        "print(f'Amount of duplicates: {df_main.duplicated().sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main info about our final data"
      ],
      "metadata": {
        "id": "-FwuxOvBFsny"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InLdwixADRpD"
      },
      "outputs": [],
      "source": [
        "df_main.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_QuZW6TlcCu"
      },
      "outputs": [],
      "source": [
        "df_main.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a column with counting days from release date"
      ],
      "metadata": {
        "id": "wMLAA16-FvPG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjKoNwX5u1xZ"
      },
      "outputs": [],
      "source": [
        "df_main['Release Date'] = pd.to_datetime(df_main['Release Date'])\n",
        "\n",
        "\n",
        "today = datetime.today().strftime('%Y-%m-%d')\n",
        "df_main['Days Since Release'] = (pd.to_datetime(today) - df_main['Release Date']).dt.days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5DngAkSvUFS"
      },
      "outputs": [],
      "source": [
        "df_main.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TyURMY_vcie"
      },
      "source": [
        "####EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatterplot of release date and days since release"
      ],
      "metadata": {
        "id": "GNi75jAIF0Wv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTCq1mqsvcUA"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig = px.scatter(df_main, x=\"Days Since Release\", y=\"Song\", color = 'Days Since Release')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Matrix"
      ],
      "metadata": {
        "id": "5KeyEIazG3X9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68pRQ40BATvF"
      },
      "outputs": [],
      "source": [
        "numerical_cols = ['duration', 'energy', 'loudness', 'speechiness',\n",
        "                  'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'danceability', 'key', 'mode']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijI0MWKMBtA0"
      },
      "outputs": [],
      "source": [
        "corr_matrix = df_main[numerical_cols].corr()\n",
        "\n",
        "plt.imshow(corr_matrix, cmap='coolwarm', interpolation='none')\n",
        "plt.colorbar()\n",
        "plt.xticks(range(len(corr_matrix)), corr_matrix.columns, rotation=90)\n",
        "plt.yticks(range(len(corr_matrix)), corr_matrix.columns)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of duration, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo and danceability by artist"
      ],
      "metadata": {
        "id": "2fSqPpa8HIuz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IklbSExsDsIp"
      },
      "outputs": [],
      "source": [
        "cols_to_compare = df_main[['duration', 'energy', 'loudness','speechiness', \n",
        "                           'acousticness', 'instrumentalness', 'liveness', \n",
        "                           'valence', 'tempo', 'danceability']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvUGrTdtOgm5"
      },
      "outputs": [],
      "source": [
        "for feature in cols_to_compare:\n",
        "    fig = px.violin(df_main, x='Artist', y=feature,  color = 'Artist', box=True, points='all')\n",
        "    fig.update_layout(title=f\"Distribution of {feature} by Artist\",\n",
        "                      xaxis_title=\"Artist\",\n",
        "                      yaxis_title=feature)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIGQ7F4KAWcZ"
      },
      "source": [
        "Scatter plot of Danceability and Valence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI4H8Ey4AZTz"
      },
      "outputs": [],
      "source": [
        "colors = np.array(df_main['Streams (Billions)'])\n",
        "plt.scatter(df_main['danceability'], df_main['valence'], c=colors, cmap='viridis')\n",
        "plt.title(\"Danceability & Valence\")\n",
        "plt.xlabel(\"Danceability\")\n",
        "plt.ylabel(\"Valence\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsgRLoTeAxEm"
      },
      "source": [
        "Scatter plot of Loudness and Energy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz6rqXwKA0lh"
      },
      "outputs": [],
      "source": [
        "colors = np.array(df_main['Streams (Billions)'])\n",
        "plt.scatter(df_main['loudness'], df_main['energy'], c=colors, cmap='viridis')\n",
        "plt.title(\"Loudness & Energy\")\n",
        "plt.xlabel(\"Loudness\")\n",
        "plt.ylabel(\"Energy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgI15M5vAZmb"
      },
      "source": [
        "Top 10 Most Streamed Artists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdSKiDl-AbcN"
      },
      "outputs": [],
      "source": [
        "# grouping by artist and summming streams and then sort \n",
        "df_grouped = df_main.groupby('Artist').agg({'Streams (Billions)': 'sum'})\n",
        "\n",
        "df_sorted = df_grouped.sort_values('Streams (Billions)', ascending=False)\n",
        "\n",
        "\n",
        "top_artists = df_sorted.head(10)\n",
        "\n",
        "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', \n",
        "          'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
        "\n",
        "# creating bar plot\n",
        "plt.barh(top_artists.index, top_artists['Streams (Billions)'], color = colors)\n",
        "plt.title('Top 10 Most Streamed Artists')\n",
        "plt.xlabel('Total Streams')\n",
        "plt.ylabel('Artist')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6Yq65f2sgSd"
      },
      "source": [
        "####Building Regression Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ip1KmeLYCas"
      },
      "source": [
        "**Models which will be used:**\n",
        "* Random Forest\n",
        "* Gradient Boosting Machines (GBM)\n",
        "* Support Vector Machines (SVM)\n",
        "* Logistic Regression\n",
        "* Neural Networks (NN)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "D-vA8gCLIQ38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsHkgyKhiZMb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "import math\n",
        "from math import sqrt\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential, load_model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfuGOh4FWglb"
      },
      "outputs": [],
      "source": [
        "df_main['Streams (Billions)'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression problem is to predict the streams of the song in billions using all the features in dataset without name of the artist, song and its release date. The target variable will be the column of streams of the song in billions."
      ],
      "metadata": {
        "id": "NGujF-YCJ6CT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rGg4DBRY-u5"
      },
      "outputs": [],
      "source": [
        "x = df_main.drop([\"Artist\", \"Streams (Billions)\", \"Release Date\", 'Days Since Release', 'id', 'Song'], axis=1)\n",
        "y = df_main[\"Streams (Billions)\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUQBtzJXh-5z"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "x_scaled = scaler.fit_transform(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx0JK0k8kn8S"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJyjY2KPk2Yk"
      },
      "source": [
        "#####**Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT6qeJBqlDEG"
      },
      "outputs": [],
      "source": [
        "rf_reg = RandomForestRegressor(random_state = 42)\n",
        "\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_reg.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "print(\"Root mean squared error:\", rmse)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared:\", r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning Random Forest Regressor using Randomized Search"
      ],
      "metadata": {
        "id": "82BIN35qIY92"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp1diXbhmJTD"
      },
      "outputs": [],
      "source": [
        "n_estimators = [int(x) for x in np.linspace(start = 200, \n",
        "                                            stop = 2000, \n",
        "                                            num = 10)]\n",
        "max_features = ['auto', 'sqrt']\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "bootstrap = [True, False]\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "\n",
        "rf_reg_tuned = RandomizedSearchCV(estimator = rf_reg, \n",
        "                                  param_distributions = random_grid,\n",
        "                                  n_iter = 100,\n",
        "                                  cv = 3,\n",
        "                                  verbose = 2,\n",
        "                                  random_state = 42,\n",
        "                                  n_jobs = -1)\n",
        "\n",
        "rf_reg_tuned.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qApnkgQMnqw"
      },
      "outputs": [],
      "source": [
        "rf_reg_tuned.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a function to evaluate models using metrics:\n",
        "* Mean Squared error\n",
        "* Mean Absolute Error\n",
        "* Root Mean squared error\n",
        "* R-squared"
      ],
      "metadata": {
        "id": "lx5l7PxqIhDQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QnaHjQ0NCf_"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_features, test_labels):\n",
        "  prediction = model.predict(test_features)\n",
        "  mse = mean_squared_error(test_labels, prediction)\n",
        "  mae = mean_absolute_error(test_labels, prediction)\n",
        "  rmse = mean_squared_error(test_labels, prediction, squared=False)\n",
        "  r2 = r2_score(test_labels, prediction)\n",
        "  print(\"Mean Squared Error:\", mse)\n",
        "  print(\"Mean Absolute Error:\", mae)\n",
        "  print(\"Root mean squared error:\", rmse)\n",
        "  print(\"R-squared:\", r2)\n",
        "  return mse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_random = rf_reg_tuned.best_estimator_"
      ],
      "metadata": {
        "id": "et7cC_HaDrsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_base = evaluate(rf_reg, X_test, y_test)"
      ],
      "metadata": {
        "id": "QOKRnir1uELT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_tuned = evaluate(best_random, X_test, y_test)"
      ],
      "metadata": {
        "id": "tZEB7jlnvCPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Improvement of {:0.2f}%.'.format( 100 * (results_base - results_tuned) / results_base))"
      ],
      "metadata": {
        "id": "-KNIsISDvNfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI8wIRuToczZ"
      },
      "source": [
        "#####**GradientBoostingMachines**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gbm_reg = GradientBoostingRegressor(random_state = 42)\n",
        "\n",
        "gbm_reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gbm_reg.predict(X_test)"
      ],
      "metadata": {
        "id": "eLq-FCDWtrFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning Gradient Boosting Regressor"
      ],
      "metadata": {
        "id": "eFJHy-z2It3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_estimators = [int(x) for x in np.linspace(start = 200, \n",
        "                                            stop = 2000, \n",
        "                                            num = 10)]\n",
        "max_features = ['auto', 'sqrt']\n",
        "learning_rate = [0.01, 0.2]\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'learning_rate': learning_rate,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               }\n",
        "\n",
        "gbm_reg_tuned = RandomizedSearchCV(estimator = gbm_reg, \n",
        "                                  param_distributions = random_grid,\n",
        "                                  n_iter = 100,\n",
        "                                  cv = 3,\n",
        "                                  verbose = 2,\n",
        "                                  random_state = 42,\n",
        "                                  n_jobs = -1)\n",
        "\n",
        "gbm_reg_tuned.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "40LucQkSuL9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_random = gbm_reg_tuned.best_estimator_"
      ],
      "metadata": {
        "id": "T6N7H-YaxveJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_base = evaluate(gbm_reg, X_test, y_test)"
      ],
      "metadata": {
        "id": "Au6kXSgkx4dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_tuned = evaluate(best_random, X_test, y_test)"
      ],
      "metadata": {
        "id": "UjKZ-6esxz2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Improvement of {:0.2f}%.'.format( 100 * (results_base - results_tuned) / results_base))"
      ],
      "metadata": {
        "id": "OuSxL22E8gBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Support Vector Machines (SVM)**"
      ],
      "metadata": {
        "id": "gErkq37383cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_reg = SVR()\n",
        "\n",
        "svm_reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_reg.predict(X_test)"
      ],
      "metadata": {
        "id": "jSES9yv6891X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning Support Vector Machine"
      ],
      "metadata": {
        "id": "C7fq3oQNIy3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "param_grid = {\n",
        "    'C': np.logspace(-3, 3, 7),\n",
        "    'epsilon': [0.1, 0.2, 0.5, 1.0],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "    }\n",
        "\n",
        "svm_reg_tuned = RandomizedSearchCV(estimator=svm_reg, \n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=100, \n",
        "                                   cv=3, \n",
        "                                   verbose=2, \n",
        "                                   random_state=42, \n",
        "                                   n_jobs=-1)\n",
        "\n",
        "\n",
        "svm_reg_tuned.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "JvU4bEpm9ujf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = svm_reg_tuned.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "y_pred = svm_reg_tuned.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "id": "0bmqyXsL-ImP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_random = svm_reg_tuned.best_estimator_\n",
        "print(best_random)"
      ],
      "metadata": {
        "id": "G2BTdufJ-cb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_base = evaluate(svm_reg, X_test, y_test)"
      ],
      "metadata": {
        "id": "c9s-G_JV-WgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_tuned = evaluate(best_random, X_test, y_test)"
      ],
      "metadata": {
        "id": "MtQwkJeL-w0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Improvement of {:0.2f}%.'.format( 100 * (results_base - results_tuned) / results_base))"
      ],
      "metadata": {
        "id": "yr1HfZxG-2nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Logistic Regression**"
      ],
      "metadata": {
        "id": "8IsZ31WcAhzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lg_reg = LogisticRegression()\n",
        "\n",
        "lg_reg.fit(X_train, y_train.astype('int'))"
      ],
      "metadata": {
        "id": "LoSKXJPOAg6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning Logistic Regression model\n"
      ],
      "metadata": {
        "id": "Swu9h1UnI2G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'C': np.logspace(-4, 4, 20),  \n",
        "    'penalty': ['l1', 'l2'],       \n",
        "    'solver': ['liblinear']        \n",
        "}\n",
        "\n",
        "lg_reg_tuned = RandomizedSearchCV(estimator=lg_reg, \n",
        "                                   param_distributions=param_grid, \n",
        "                                   n_iter=100,  \n",
        "                                   cv=5,        \n",
        "                                   random_state=42,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "lg_reg_tuned.fit(X_train, y_train.astype('int'))"
      ],
      "metadata": {
        "id": "FaKHPCgqurQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = lg_reg_tuned.best_params_\n",
        "best_random = lg_reg_tuned.best_estimator_\n",
        "print(best_params)\n",
        "print(best_random)"
      ],
      "metadata": {
        "id": "08trqYc5v0-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lg_reg_tuned = LogisticRegression(penalty = 'l1',\n",
        "                                  C = 0.23357214690901212,\n",
        "                                  solver = 'liblinear')"
      ],
      "metadata": {
        "id": "VzIYlUSW0sRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lg_reg_tuned.fit(X_train, y_train.astype('int'))\n",
        "\n",
        "results_tuned = evaluate(lg_reg_tuned, X_test, y_test)"
      ],
      "metadata": {
        "id": "dOeTLZdm1C24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_base = evaluate(lg_reg, X_test, y_test)"
      ],
      "metadata": {
        "id": "qFfWjKmi1HBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Improvement of {:0.2f}%.'.format( 100 * (results_base - results_tuned) / results_base))"
      ],
      "metadata": {
        "id": "qaWMbyqx2IYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Neural Network"
      ],
      "metadata": {
        "id": "XifglaCl2NE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating base network"
      ],
      "metadata": {
        "id": "NCKe3_NlI8eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1))  \n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "mse = model.evaluate(X_test, y_test)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "T4NfvM1s2Obj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning Neural Network"
      ],
      "metadata": {
        "id": "HaessIYuraDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_tuned = Sequential()\n",
        "model_tuned.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))  \n",
        "model_tuned.add(Dropout(0.2))  # Dropout layer to prevent overfitting\n",
        "model_tuned.add(Dense(64, activation='relu'))  # Hidden layer\n",
        "model_tuned.add(Dropout(0.2))  \n",
        "model_tuned.add(Dense(1, activation='linear'))  \n",
        "\n",
        "model_tuned.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
        "\n",
        "history = model_tuned.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "mse_tuned = model_tuned.evaluate(X_test, y_test)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "id": "MQp37sNACBZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mse)"
      ],
      "metadata": {
        "id": "-TeNN6zvD8zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mse_tuned)"
      ],
      "metadata": {
        "id": "JwdIDV81D-YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Improvement of {:0.2f}%.'.format(100 * (mse - mse_tuned) / mse))"
      ],
      "metadata": {
        "id": "e_2PvYeCCob3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####What model is the best?"
      ],
      "metadata": {
        "id": "AVcObcQnDKjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse_values = [0.20920275288513707, 0.21675758635354847, 0.25346728762066706, 1.07842985, 0.6379622220993042]  \n",
        "\n",
        "\n",
        "model_names = ['RF', 'GBM', 'SVM', 'LR', 'NN']  \n",
        "\n",
        "\n",
        "plt.bar(model_names, mse_values)\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Comparison of MSE Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6lgCwsizDNY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Conclusion"
      ],
      "metadata": {
        "id": "C8KzsF4_JKxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the best model to use with a regression problem (predicting Streams in billions of top songs) on this dataset is **Random Forest** with the lowest MSE, the worst is **Logistic Regression**.\n",
        "\n",
        "ðŸ˜€\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IDy5EicsJMs5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}